---
title: "Selected StatLab Consultation Code Excerpts"
author: "Jacob Goldstein-Greenwood ([email](mailto:jacobgg@virginia.edu))"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document contains excerpts from code I prepared for a few of the statistical and programming consultations I've provided as part of my work for the UVa Library's StatLab. I've removed identifying information from code chunks and comments. Some consultations are handled solely through email, although most involve a meeting at which I learn more about the inquirer's question or problem, discuss possible solutions, and walk through any code I've drafted with them.

---

<details><summary>Calculating the entropy of images</summary><br/>

I received a consultation request from someone interested in quantifying and comparing the amount of disorder in images of greenspaces. I proposed that the inquirer could calculate the information entropy of each image's pixel intensity values. I prepared a function to read in images, extract pixel-wise grayscale values, and calculate information entropy from a binned version of those values.

```{r, echo = T, eval = F}
# Relevant packages
library(CulturalAnalytics)
library(jpeg)

# A function to (a) read in an image as an array of values, (b) convert the array to grayscale, (c) generate a histogram of grayscale (i.e., intensity) values, and (d) produce the entropy of those values (more details on the specifics of this function are at the bottom of this file):
img_entropy <- function(your_image) {
  img <- jpeg::readJPEG(your_image)
  img_intensity <- CulturalAnalytics::imageToIntensity(img, method = 'mean')
  img_intensity_histogram <- hist(img_intensity, breaks = 0:255/255)
  CulturalAnalytics::imageEntropy(img_intensity_histogram)
}

# Running the function on a pair of sample images to demonstrate:
img_entropy('low_comp.jpg')
img_entropy('high_comp.jpg')

# Once you have a full set of masked images, you can get entropy estimates for each of them all at once by running the following, which iterates the function above through a directory of images:
entropy_values <- sapply(dir('/Users/username/Desktop/image_folder'), img_entropy)
entropy_values

# Additional info on the img_entropy function I've written above:
# readJPEG() reads in an image as an array of values
# imageToIntensity converts that array to grayscale by taking the average of the RGB channels (use View(imageToIntensity) to see the source code in detail); note that taking the average seems like a fine option, but there are alternatives, and it's possible that a luminosity-preserving method might be preferred, see here: https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/
# hist() the generates a histogram from those intensity values; I've currently set the histogram to have 256 breaks ranging from 0 to 1 (i.e., breaks = 0:255/255; my understanding is that grayscale data is commonly treated as 256 shades from 0 to 1)
# imageEntropy() calculates Shannon entropy from that histogram per the following formula (use View(imageEntropy) to see the source code):
#     -sum(p(x_i) * log2(p(x_i)))
#     Where p(x_i) is the proportion (probability) of cases falling in a given intensity bin. imageEntropy() uses log base-2, and entropy is therefore in bits
```

</details>

---

<details><summary>Identifying data mismatches across elements of large lists</summary><br/>

I received a consultation request from someone looking to clean data contained in two large lists of data frames. Each list element contained a variable number of years of data on a certain measure (one list housed data frames containing yearly values on one measure, X; the other list housed data frames containing values on a different measure, Y). The two lists were the same length. The inquirer was looking to identify which matched pairs of list elements contained *mismatched* years of data (e.g., the i^th^ element of List 1 contained X values for 2000:2015, but the i^th^ element of List 2 only contained Y values 2000:2008 and 2011:2015). When year mismatches were identified for a given pair of list elements, the inquirer was looking to drop those years from the data frames. I proposed the following:

```{r, echo = T, eval = F}
library(rlist)
library(psych)

list1 <- list.load("list1", type = "RData")
list2 <- list.load("list2", type = "RData")

start_clean <- Sys.time()
for (i in 1:length(list1)) {
  if(all(is.element(list1[[i]]$Year, list2[[i]]$Year)) & all(is.element(list2[[i]]$Year, list1[[i]]$Year)) == T) {
    next
  } else {
    years_missing_from_list1 <- list2[[i]]$Year[which(list2[[i]]$Year %in% list1[[i]]$Year == F)]
    years_missing_from_list2 <- list1[[i]]$Year[which(list1[[i]]$Year %in% list2[[i]]$Year == F)]
    years_to_cast_out <- c(years_missing_from_list1, years_missing_from_list2)
    list1[[i]] <- list1[[i]][!(list1[[i]]$Year %in% years_to_cast_out), ]
    list2[[i]] <- list2[[i]][!(list2[[i]]$Year %in% years_to_cast_out), ]
  }
}
fin_clean <- Sys.time()
cat('Loop took', round(fin_clean - start_clean, digits = 2), 'seconds to run')
```

</details>

---

<details><summary>Calculating and structuring transition matrices for data across time</summary><br/>

I received an inquiry from someone interested in calculating transition matrices for observations that were each measured at five time points on a scale from 0:3. The inquirer had a function in hand for calculating transition matrices for single observations---`Markovmatrix2` below---but the function wasn't acting as desired: It was omitting rows and columns of the matrices that were needed for the inquirer's intended analysis (example provided in the code comments below). I proposed the following solution:

```{r, echo = T, eval = F}
# Existing function for generating transition matrices
Markovmatrix2 <- function(X, l = 1) {
  tt <- table(X[, -c((ncol(X)-l+1):ncol(X))], c(X[ , -c(l:1)]))
  tt <- tt / sum(tt)
  tt
}

# Problem: Markovmatrix2 by itself won't generate a transition matrix with the same ncol and nrow for each observation
#   E.g., if an observation's values are c(0, 2, 2, 2, 0) (i.e., no score of 1 or 3 at any time point), the output is a 2x2 matrix with 0/2 along the rows and 0/2 along the columns, e.g.:
#       0       2
#   0   0.00    0.25
#   2   0.25    0.50
# Each matrix, however, needs to be 4x4, with 0:3 along the rows and the columns, e.g.:
#       0       1       2       3       
#   0   0.00    0.00    0.25    0.00
#   1   0.00    0.00    0.00    0.00
#   2   0.25    0.00    0.50    0.00
#   3   0.00    0.00    0.00    0.00
# I wrote the function below that checks which rows and columns (if any) are missing for each transition matrix and adds them in as necessary

tran_mat <- function(df) {
  # Cautionary note: tran_mat() is only designed to generate transition matrices for observations with scores in the 0:3 range
  out <- vector(mode = 'list', length = nrow(df))
  intended_row_count <- 0:3
  intended_col_count <- 0:3
  for (i in 1:nrow(df)) {
    # Get Markovmatrix2() output for the i_th row
    temp <- Markovmatrix2(as.matrix(df[i, ]))
    temp.df <- as.data.frame.matrix(temp)
    # See if any rows or columns, from 0:3, are missing for the i_th row
    rows_to_add <- intended_row_count[-which(0:3 %in% rownames(temp.df))]
    cols_to_add <- intended_col_count[-which(0:3 %in% colnames(temp.df))]
    # Add missing rows, if present
    if (length(rows_to_add) > 0) {
      for (r in 1:length(rows_to_add)) {
        temp.df[as.character(rows_to_add[r]), ] <- 0
      }
    }
    # Add missing columns, if present
    if (length(cols_to_add) > 0) {
      for (c in 1:length(cols_to_add)) {
        temp.df[, as.character(cols_to_add[c])] <- 0
      }
    }
    # Order rows/columns correctly
    temp.df <- temp.df[c('0', '1', '2', '3'), c('0', '1', '2', '3')]
    # Save output to appropriate position in list
    out[[i]] <- temp.df
  }
  out
}

# Simulate data with scores from 0:3 at five time points
set.seed(10)
scores <- 0:3
df <- data.frame(t1 = sample(scores, 100, replace = T),
                 t2 = sample(scores, 100, replace = T),
                 t3 = sample(scores, 100, replace = T),
                 t4 = sample(scores, 100, replace = T),
                 t5 = sample(scores, 100, replace = T))

# Run with simulated data
example_results_list <- tran_mat(df)
```

</details>

---

<details><summary>Clustering standard errors in education data</summary><br/>

I received a consultation request from someone looking to analyze the effect of an education intervention in repeated-measures data. Each participant had two or three associated measurements, with a (categorical) intervention value and an outcome measure (an exam score) listed at each time point. I generally incline toward using mixed-effects models to handle data of this sort, but in this case, to accommodate the inquirer's particular level of comfort with some statistical methods and not others, I proposed accounting for the dependent structure of the data using clustered standard errors. I simulated a dataset and worked up this example:

```{r, echo = T, eval = F}
# Clustering standard errors to account for dependant data in a one-way ANOVA

######## Simulate some data
set.seed(99)
# Generate participant IDs; some participants have two associated observations; some have three
participant_id <- rep(1:500, sample(2:3, 500, replace = T))
num_ps <- length(participant_id)
# Generate (categorical) intervention value for each participant (designated by letters) and exam scores
faux_exam_data <- data.frame(participant_id,
                             timer_method = factor(sample(LETTERS[1:10], num_ps, replace = T)),
                             score = round(rnorm(length(participant_id), mean = 75, sd = 10), digits = 2))
# Convert any scores over 100 to 100
faux_exam_data$score <- ifelse(faux_exam_data$score > 100, 100, faux_exam_data$score)
# Take a look at the data
head(faux_exam_data)

######## Unmodified one-way ANOVA
anova_mod <- lm(score ~ timer_method, data = faux_exam_data)
# If we give this model to the function `glht()` from the `multcomp` package, we can perform multiple-comparison tests examining the difference between each pair of timer types for significance
# Note: glht() performs a p-value correction to account for the fact that performing multiple tests can inflate the false-discovery rate
library(multcomp)
unmodified_anova_comparisons <- glht(anova_mod, linfct = mcp(timer_method = 'Tukey'))
summary(unmodified_anova_comparisons)

####### However, we know that the data are *dependent*: Not every observation is independent of each other, because each participant contributes either 2 or 3 scores to the data (i.e., there are clusters in the data)
# Non-independence can lead us to underestimate standard errors
# We can use clustered standard errors to try and account for the clustered structure of the data
# The `sandwich` package provides a function for generating a "clustered covariance matrix", vcovCL(), and we can pass that to glht() to get pairwise comparisons between timer types that use clustered standard errors
library(sandwich)
clustered_se_comparisons <- glht(anova_mod, linfct = mcp(timer_method = 'Tukey'),
                                 vcov. = vcovCL(anova_mod, cluster = faux_exam_data$participant_id))
summary(clustered_se_comparisons)
```

</details>

---

<details><summary>Calculating the Bray-Curtis Dissimilarity Index</summary><br/>

I received an inquiry from someone looking use R to calculate the Bray-Curtis dissimilarity index, which provides a quantitative measure of the dissimilarity in the compositions of two sites/groups.

$$BC_{ij} = 1 - \frac{2C_{ij}}{S_i + S_j}$$

Where:

- $C_{ij}$ is the sum of the lesser counts for species/types that are observed at *both* sites
- $S_i$ is the total count of observations at site $i$
- $S_j$ is the total count of observations at site $j$

In effect, the fraction component of the formula reflects the proportion of all observations (from both sites) that have a same-species/same-type "match" at the other site. $BC_{ij}$, then is the proportion of all observations that *do not* have a match at the other site.

I simulated some relevant data and worked up the following example:

```{r, echo = T, eval = F}
set.seed(150)

trees <- data.frame(plot = c(rep('highlands', 10), rep('lowlands', 10)),
                    species = c(c('fir', 'birch', 'oak', letters[1:7]),
                                c('fir', 'birch', 'oak', letters[20:26])),
                    count = sample(1:10, 20, replace = T))
# Shared species at both plots: Fir, birch, oak

##### Bray-Curtis dissimilarity index:
# 1. Take two times the sum of lesser counts for shared species
# Higlands:
#   Fir: 3 [LESSER]
#   Birch: 3 [GREATER]
#   Oak: 1 [LESSER]
# Lowlands:ccc
#   Fir: 4 [GREATER]
#   Birch: 2 [LESSER]
#   Oak: 7 [GREATER]
lesser_sum <- 2 * (3 + 2 + 1)
# 2. Divide by sum of total counts at both plots; then subtract from 1 to make higher values means more dissimilar
plot_counts <- aggregate(trees$count, by = list(trees$plot), sum)
bc_dissim <- 1 - (lesser_sum / sum(plot_counts$x))
bc_dissim

# Equation: https://www.statisticshowto.com/bray-curtis-dissimilarity/
# The fraction part of the equation answers the following question: "Out of the total number of trees across the plots, what proportion of trees have a one-to-one match with a same-species tree at the other site?" (If there are 10 pines in the highlands and 8 pines in the lowlands, we only consider there to be 8*2 = 16 matched pines, not 10 + 8 = 18.)
```

</details>

---

<details><summary>Using regex to extract portions of records for text analysis</summary><br/>

I received an inquiry from someone who had a number of records that they were attempting to perform text analysis on. However, each record needed to be trimmed down to only include the text in each record's "SUMMARY" section (at the bottom of each document). I proposed resolving this with a simple positive lookbehind regex:

```{r, echo = T, eval = F}
library(stringi)

# Goal: Select all text from "SUMMARY:" onward
sample_text <- c('Text before the section of interest.\nMore text before the section of interest.\nSUMMARY:\n1. Section of interest.\n2. More of section of interest.')

# First, remove "\n" (new line) markers, as regex pattern "." cannot match "\n"
sample_text <- gsub('\n', ' ', sample_text)

# Extract with a positive lookbehind
stri_extract(sample_text, regex = '(?<=SUMMARY:)(.*)')

# Or, if including "SUMMARY:" in the extraction is desired
stri_extract(sample_text, regex = '(SUMMARY:)(.*)')
```

</details>